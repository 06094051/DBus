input {
    file {
        #所要读取的日志文件路径
        path => ["/app/dbus/dbus-heartbeat-0.4.0/logs/heartbeat/*.*"]
    	sincedb_path => "/app/dbus/logstash-5.6.1/etc/sincedb_heartbeat"
	codec => multiline {
      	     pattern => "^\["  #根据所要合并的多行进行设置（此处为正则表达式，表示以[为开头的行才作为一条记录，其余的向前合并）
	     negate => "true"
      	     what => "previous" #表示不符合上述pattern的行，向前合并
    	}
        type => "heartbeat_log_logstash"  	#改成相应的数据源名
        start_position => "end"  	#表示从什么位置开始读取文件数据，默认是结束位置；此处设置beginning意为从头开始读取
	#close_older => 3600		#fd 无数据关闭文件时间间隔默认 1小时 3600
	#max_open_files => 4095		#运行打开最大文件上线，默认 4095
        #add_field => {"test"=>"test"}  #添加自定义的字段   未使用
        #tags => "tag1"			#增加标签           未使用
        #discover_interval => 15	#设置多长时间扫描目录，发现新文件  使用默认即可
        #stat_interval => 1		#设置多长时间检测文件是否修改	   使用默认即可
        #sincedb_write_interval => 15   # sincedb 写文件时间频率 默认 15秒
    }
    #logstash本身的心跳信息
    heartbeat {
        message => "epoch"
        interval => 60
        type => "dbus-heartbeat"
    }
}

filter {
	if [type] == "heartbeat_log_logstash" { #与上面input file中的type名对应，意为对上述input file中取得的数据进行进一步过滤抽取
       		 grok {
            		patterns_dir => "../patterns" #此处为自定义的pattern，如果要添加或修改自定义的pattern,可以放在此目录下。下面HEARTBEATTIMESTAMP即是定义在此目录下的
           		 match => {
            		#logstash本身配置了约120种pattern，其中DATA、LOGLEVEL和GREEDYDATA都在其中，详细参考https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns
                 	 "message" => "\[%{DATA:thread}\] %{LOGLEVEL:level} ?\: %{HEARTBEATTIMESTAMP:timestamp}%{GREEDYDATA:log}"
            		 }
       		 }	
       }
}
output {
    stdout{codec=>rubydebug}
    #代表输出到kafka，以下要配置kafka的服务器IP，端口，topic id
    kafka {
	    bootstrap_servers => "dbus-n1:9092,dbus-n2:9092,dbus-n3:9092"  #kafka 服务器IP:端口
            topic_id => "heartbeat_log_logstash"  #要输出到kafka的topic id
	    acks => "all"
	    compression_type => "lz4"
   	    retries => 3
	    codec => json {
		charset => "UTF-8"
	    } 
	    batch_size => 1048576    	  # batch max than 1MB size, larger than it, send batch
	    linger_ms => 1000        	  # batch wait 1 seconds
	    max_request_size => 10485760  # set as 10MB, max_request_size => Default value is 1048576
	    buffer_memory => 67108864     # #default size 33554432=32M. 67108864=64M 

	    #message_key => "heartbeat-logstash"
	    # max_request_size => Default value is 1048576
	    # send_buffer_bytes => Default value is 131072
	    # key_serializer => Default value is "org.apache.kafka.common.serialization.StringSerializer"
	    # value_serializer => Default value is "org.apache.kafka.common.serialization.StringSerializer"
	 }
}
